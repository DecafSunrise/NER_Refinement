{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summary Tool\n",
    "\n",
    "There are two kinds of summaries - **\"Extractive\"**, and **\"Abstractive\"**.\n",
    "\n",
    "This tool is an **Extractive** summary tool; That means it simply selects the \"most important\" sentences from a body of text and returns them. Computers are dumb though - There's no guarantee that this is a good summary. In our case, this tool counts how often each word is included in the text, then assigns a weight based on how often a word is used. If a sentence uses commonly-used words often, it'll likely score higher, and be returned by this tool. There are some unintended consequences too: longer sentences will be ranked more highly than shorter sentences.\n",
    "\n",
    "The way humans summarize text, you're synthesizing new content based on input. That's an abstractive summary, and not how this tool works. If that's something you're interested in, I'd recommend looking into Google Research's PEGASUS model.\n",
    "\n",
    "Code borrowed from:\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "\n",
    "Another reference:\n",
    "https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities\n",
    "\n",
    "This tool can ingest the following file formats:\n",
    "- .txt\n",
    "- .pdf\n",
    "- .doc, .docx\n",
    "- Limited url support.\n",
    "    - The tool will try to scrape your target site, but will not return particularly helpful messages if the requests fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "import heapq\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import textract\n",
    "\n",
    "import docx # This is whacky, but it's how you import python-docx. \n",
    "#pip install docx will install the wrong thing though\n",
    "from docx import Document\n",
    "from docx.shared import Pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to add iPython widget support\n",
    "Commented out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "# def interactiveBoxes(k, article):\n",
    "#     summarize(k, article)\n",
    "    \n",
    "# iplot = interact(interactiveBoxes, n = widgets.Text(value='20',\n",
    "#     placeholder='Type something', description='Total #:',disabled=False),\n",
    "#         article = widgets.Text(value='https://en.wikipedia.org/wiki/Abstract_(summary)',\n",
    "#     placeholder='Type something', description='Highlight:',disabled=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPDFtext(filename):\n",
    "    pdf_text = textract.process(filename)\n",
    "    if isinstance(pdf_text, (bytes, bytearray)):\n",
    "        pdf_text = pdf_text.decode(\"utf-8\")\n",
    "    \n",
    "    return pdf_text\n",
    "\n",
    "def getDocXtext(filename):\n",
    "    ## Dumps the text of your word doc\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return ' '.join(fullText)\n",
    "\n",
    "def getUrlText(filename):\n",
    "    scraped_data = urllib.request.urlopen(filename)\n",
    "    article = scraped_data.read()\n",
    "\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "    webtext = \"\"\n",
    "\n",
    "    for p in paragraphs:\n",
    "        webtext += p.text\n",
    "    return webtext\n",
    "\n",
    "\n",
    "def getText(filename):\n",
    "    #Split the file once on a period, starting from the rear, then grab the last entry in the resultant list\n",
    "    filetype = filename.split(\".\",-1)[-1].lower()\n",
    "    article_text = \"\"\n",
    "    if filetype == \"pdf\":\n",
    "        try:\n",
    "            print(\"Looks like a PDF\")\n",
    "            article_text = getPDFtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "    elif filetype in [\"doc\", \"docx\"]:\n",
    "        try:\n",
    "            print(\"Looks like a Word Doc\")\n",
    "            article_text = getDocXtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filetype == \"txt\":\n",
    "        try:\n",
    "            print(\"Looks like a .txt doc\")\n",
    "            article_text = open(filename, \"r\").read()\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filename.startswith(\"http\"):\n",
    "        print(\"Looks like a link!\")\n",
    "        try:\n",
    "            article_text = getUrlText(filename)\n",
    "        except:\n",
    "            print(\">>Couldn't grab text\")\n",
    "    else:\n",
    "        print(\"\\t>>Not sure what kind of file that is!\")\n",
    "    print(f\"\\t>>{len(nltk.word_tokenize(mysummary))} words\\n\\t>>{len(nltk.sent_tokenize(mysummary))} sentences\")\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(k, article):\n",
    "    article_text = getText(article)\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    \n",
    "#     formatted_article_text = article_text.replace(\"\\t\", ' ', formatted_article_text)\n",
    "        \n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "                        \n",
    "    summary_sentences = heapq.nlargest(k, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    # print(summary)\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "    for doc in nltk.sent_tokenize(summary):\n",
    "        print(\"• \"+doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like a link!\n",
      "\t>>340 words\n",
      "\t>>11 sentences\n",
      "• The descriptive abstract, also known as the limited abstract or the indicative abstract, provides a description of what the paper covers without delving into its substance.\n",
      "• Abstract is often expected to tell a complete story of the paper, as for most readers, abstract is the only part of the paper that will be read.\n",
      "• An abstract may or may not have the section title of \"abstract\" explicitly listed as an antecedent to content.\n",
      "• When used, an abstract always appears at the beginning of a manuscript or typescript, acting as the point-of-entry for any given academic paper or patent application.\n",
      "• It is generally agreed that one must not base reference citations on the abstract alone, but the content of an entire paper.\n",
      "• In management reports, an executive summary usually contains more information (and often more sensitive information) than the abstract does.\n",
      "• [citation needed] Abstracts in which these subheadings are explicitly given are often called structured abstracts.\n",
      "• Moreover, some journals also include video abstracts and animated abstracts made by the authors to easily explain their papers.\n",
      "• An abstract may act as a stand-alone entity instead of a full paper.\n",
      "• Usually between 100 and 200 words, the informative abstract summarizes the paper's structure, its major topics and key points.\n",
      "• Abstracts that comprise one paragraph (no explicit subheadings) are often called unstructured abstracts.\n",
      "• [citation needed] However, publishers of scientific articles invariably make abstracts freely available, even when the article itself is not.\n",
      "• The terms précis or synopsis are used in some publications to refer to the same thing that other publications might call an \"abstract\".\n",
      "• Abstracts are typically sectioned logically as an overview of what appears in the paper, with any of the following subheadings: Background, Introduction, Objectives, Methods, Results, Conclusions.\n",
      "• A format for scientific short reports that is similar to an informative abstract has been proposed in recent years.\n",
      "• Academic literature uses the abstract to succinctly communicate complex research.\n",
      "• Once papers are chosen based on the abstract, they must be read carefully to be evaluated for relevance.\n",
      "• Most bibliographic databases only index abstracts rather than providing the entire text of the paper.\n",
      "• Abstract: Background Drafting in cetaceans is defined as the transfer of forces between individuals without actual physical contact between them.\n",
      "• Non-literary documents were also abstracted: the Tebtunis papyri found in the Ancient Egyptian town of Tebtunis contain abstracts of legal documents.\n"
     ]
    }
   ],
   "source": [
    "summarize(20, \"https://en.wikipedia.org/wiki/Abstract_(summary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtests",
   "language": "python",
   "name": "nlptests"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
